{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>BUSS6002 - Data Science in Business</h1></center>\n",
    "\n",
    "#### Pre-Tutorial Checklist\n",
    "\n",
    "1. Complete all tasks from week 9\n",
    "2. Read up to exercise 1\n",
    "3. Install mockr library\n",
    "\n",
    "\n",
    "# Tutorial 10 - MapReduce\n",
    "\n",
    "\n",
    "## What is MapReduce? \n",
    "\n",
    "MapReduce is a programming model that allows us to perform calculations in parallel rather than in series, so that instead of requiring a supercomputer to iterate through all calculations extremely fast, we can instead have lots of regular processors working on parts of a problem at the same time. MapReduce is composed of two steps. The Mapping phase breaks the problem into smaller independent calculations that can be solved individually and maps them to queues. The Reducing phase combines the outcomes of these smaller computations to arrive at a solution for the original task. MapReduce is most often used for processing data, however it is flexible enough that it can be used for more complex tasks such as linear regression, which we will demonstrate.\n",
    "\n",
    "A MapReduce program consists of two primary steps:\n",
    "\n",
    "**Map**, performs filtering and sorting into queues\n",
    "\n",
    "**Reduce**, performs a summary operation on a queue\n",
    "\n",
    "<img src=\"img/mapreduce.gif\">\n",
    "\n",
    "*Image sourced from https://www.ibm.com/developerworks/cloud/library/cl-openstack-deployhadoop/*\n",
    "\n",
    "## Why Do We Need MapReduce?\n",
    "\n",
    "As the volume of data acquired by businesses and individuals continues to grow it becomes problematic to store and process this data rapidly enough for useful insights. Most naive algorithms cannot deal with such volumes of data, either due to requiring too much computation time or memory.\n",
    "\n",
    "The \"divide and conquer\" style of MapReduce has the ability to resolve this issue. By carefully decomposing the problem we can distribute the processing load over many machines. This is known as horizontal scaling. This distrubuted computing style is more robust as we are no longer reliant on one large computer for our processing. MapReduce implementations can be designed in a fault tolerant way so that machines can be disabled or disconnected without affecting results. Additionally the hardware requirements of each machine is reduced.\n",
    "\n",
    "## MapReduce in Python\n",
    "\n",
    "There are many MapReduce libraries in Python. We will use \"mockr\", which is a very simple library.\n",
    "\n",
    "mockr allows us to write our map and reduce functions in normal Python. You can install it via:\n",
    "\n",
    "    pip install mockr\n",
    "    \n",
    "<div style=\"margin-bottom: 0px;\"><img width=20 style=\"display: block; float: left;  margin-right: 20px;\" src=\"img/docs.png\"> <h3 style=\"padding-top: 0px;\">Documentation - mockr</h3></div>\n",
    "https://pypi.org/project/mockr/\n",
    "    \n",
    "\n",
    "## Example: Counting Words\n",
    "\n",
    "Lets start by using the canonical MapReduce example of counting words in a corpus.\n",
    "\n",
    "First we need to define the map and reduce functions. Then we can run the mapreduce algorithm to count the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 1), ('this', 1), ('is', 2), ('a', 1), ('sample', 1), ('string', 1), ('it', 1), ('very', 1), ('simple', 1), ('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from mockr import run_stream_job\n",
    "\n",
    "# This regular expression matches words\n",
    "# Test it at https://www.regexpal.com\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "def map_fn(chunk):\n",
    "    # Use the regex to find all words in each chunk\n",
    "    # The chunk is a line of text because we are\n",
    "    # using run_stream_job\n",
    "    for word in WORD_RE.findall(chunk):\n",
    "        # Emit a result using the word as the key and\n",
    "        # the number of times it occured. We emit once\n",
    "        # for each word so this value is 1.\n",
    "        yield (word.lower(), 1)\n",
    "\n",
    "def reduce_fn(key, values):\n",
    "    # Recieves all the values for each key (unique word)\n",
    "    # then sums them together for the total count\n",
    "    yield (key, sum(values))\n",
    "\n",
    "# \"\\n\" is the newline character which seperates the lines of text\n",
    "input_str = \"Hello!\\nThis is a sample string.\\nIt is very simple.\\nGoodbye!\"\n",
    "\n",
    "# run_stream_job expects a newline delimited string, map function and reduce function\n",
    "# and returns a list of results\n",
    "# https://mockr.readthedocs.io/en/latest/api.html#mockrmockmr.run_stream_job\n",
    "results = run_stream_job(input_str, map_fn, reduce_fn)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yield\n",
    "\n",
    "The yield keyword returns an ephemeral object. Once it is read by any other code it gets deleted immediately. We use yield so that the data is guaranteed to only be read by one of the reducers.\n",
    "\n",
    "<div style=\"margin-bottom: 30px;\"><img width=48 style=\"display: block; float: left;  margin-right: 20px;\" src=\"img/question-mark-button.png\"> <h3 style=\"padding-top: 15px;\">Exercise 1 - Wordcount MapReduce</h3></div>\n",
    "\n",
    "Given a line of text that's n words long with k unique words as input:\n",
    "- How many keys are returned by the `map_fn` function?\n",
    "- How many keys are returned after the output from `map_fn` has been passed through `reduce_fn`?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "1. Each line of text is sent to a mapper\n",
    "\n",
    "\n",
    "2. Each mapper:\n",
    "    - Splits the line into words\n",
    "    - For each word: yields a key/value pair\n",
    "    - Key: word\n",
    "    - Value: number of times it occurs (since we iterate over all words we set this to 1)\n",
    "\n",
    "\n",
    "3. Mapped key/value pairs are sent to shuffler\n",
    "\n",
    "\n",
    "4. Key/value pairs are shuffled\n",
    "    - For each key collect the corresponding list of values\n",
    "\n",
    "\n",
    "5. Key/values are routed to respective reducers\n",
    "    - Each reducer only works on a single key\n",
    "\n",
    "\n",
    "6. Reduce\n",
    "    - Sum the counts for a key\n",
    "\n",
    "## Simulation of MapReduce Stages\n",
    "\n",
    "Below is a simulation of the output at each stage.\n",
    "\n",
    "### Text is broken into lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'This is a sample string.', 'It is very simple.', 'Goodbye!']\n"
     ]
    }
   ],
   "source": [
    "input_str = \"Hello!\\nThis is a sample string.\\nIt is very simple.\\nGoodbye!\"\n",
    "\n",
    "# Split the string into lines and store in a list\n",
    "lines_of_text = input_str.split(\"\\n\")\n",
    "\n",
    "print(lines_of_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "\n",
    "We need to apply map_fn to each line of text and collect the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('hello', 1)], [('this', 1), ('is', 1), ('a', 1), ('sample', 1), ('string', 1)], [('it', 1), ('is', 1), ('very', 1), ('simple', 1)], [('goodbye', 1)]]\n"
     ]
    }
   ],
   "source": [
    "# We will store the output of map_fn in here\n",
    "word_count_lists = []\n",
    "\n",
    "# For every line of text\n",
    "for line in lines_of_text:\n",
    "    # Apply the map function (split and count words)\n",
    "    # Save the result as a list in our list\n",
    "    word_count_lists.append(list(map_fn(line)))\n",
    "\n",
    "# Show the result of mapping\n",
    "print(word_count_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nested list of lists, which we can flatten into a normal list using itertools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 1), ('this', 1), ('is', 1), ('a', 1), ('sample', 1), ('string', 1), ('it', 1), ('is', 1), ('very', 1), ('simple', 1), ('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# word_count_lists is a list of lists\n",
    "# Flatten the list of words to make it simpler by chaining lists together\n",
    "word_count_list_flat = list(itertools.chain.from_iterable(word_count_lists))\n",
    "\n",
    "print(word_count_list_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and Sort\n",
    "\n",
    "This stage is taken care of by mockr but it is important to understand!\n",
    "\n",
    "We need to group all the counts for each word together. We will store this in a dictionary (key/value data type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello: [('hello', 1)]\n",
      "this: [('this', 1)]\n",
      "is: [('is', 1), ('is', 1)]\n",
      "a: [('a', 1)]\n",
      "sample: [('sample', 1)]\n",
      "string: [('string', 1)]\n",
      "it: [('it', 1)]\n",
      "very: [('very', 1)]\n",
      "simple: [('simple', 1)]\n",
      "goodbye: [('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "# SHUFFLE/SORT STAGE\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a dictionary where the default value is a list\n",
    "word_tuple_dict = defaultdict(list)\n",
    "\n",
    "for kv_pair in word_count_list_flat:\n",
    "    # For each unique key append the (word, count) tuple to that keys list\n",
    "    word_tuple_dict[kv_pair[0]].append(kv_pair)\n",
    "\n",
    "# Print it in a nice format:\n",
    "for k, v in word_tuple_dict.items():\n",
    "    print(str(k) +\": \" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "\n",
    "Now we need to apply the reduce_fn to our sorted data. Collect all counts for each word into a list and send this to the reduce_function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('hello', 1)], [('this', 1)], [('is', 2)], [('a', 1)], [('sample', 1)], [('string', 1)], [('it', 1)], [('very', 1)], [('simple', 1)], [('goodbye', 1)]]\n"
     ]
    }
   ],
   "source": [
    "# REDUCE STAGE\n",
    "results = []\n",
    "\n",
    "for k, v in word_tuple_dict.items():\n",
    "    # Get the counts from the list of k/v pairs\n",
    "    vals_list = [t[1] for t in v]\n",
    "    \n",
    "    # Apply the reduce_fn to the word and counts pair\n",
    "    # reduce_fn will yield a (key, value) tuple\n",
    "    # inside a generator object which we convert to a list\n",
    "    results.append(list(reduce_fn(k, vals_list)))\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we flatten the list using itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 1), ('this', 1), ('is', 2), ('a', 1), ('sample', 1), ('string', 1), ('it', 1), ('very', 1), ('simple', 1), ('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Flatten the results to make them more readable\n",
    "results_flat = list(itertools.chain.from_iterable(results))\n",
    "\n",
    "print(results_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data Set\n",
    "\n",
    "This was a small example. Let's look at a bigger example: counting all the words in Moby Dick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 14697), ('project', 91), ('gutenberg', 92), ('ebook', 10), ('of', 6742), ('moby', 89), ('dick', 88), ('or', 797), ('whale', 1101), ('by', 1222), ('herman', 4), ('melville', 4), ('this', 1438), ('is', 1746), ('for', 1643), ('use', 49), ('anyone', 6), ('anywhere', 16), ('at', 1335), ('no', 591)]\n"
     ]
    }
   ],
   "source": [
    "from mockr import run_stream_job\n",
    "\n",
    "# Open the MobyDick text file\n",
    "input_file = open(\"MobyDick.txt\", 'r')\n",
    "\n",
    "# Read the file as a string\n",
    "input_str = input_file.read()\n",
    "\n",
    "# Run map reduce using the same map_fn and reduce_fn that we defined before\n",
    "results = run_stream_job(input_str, map_fn, reduce_fn)\n",
    "\n",
    "# Show the first 20 results\n",
    "print(results[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling \n",
    "\n",
    "Imagine we want to perform this same process on all the tweets ever generated. Or on all the text on the web. Can we do this with your personal computer in a reasonable time? No! We would need to use a cluster of computers (workers) and map-reduce.\n",
    "\n",
    "With the previous word count example we could send any line of text to any map worker in the cluster. Then we can have a set of reduce workers dedicated to one or many keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Linear Regression\n",
    "\n",
    "Many machine learning problems can be decomposed into sub problems that can be solved independently. This means we can use map-reduce to solve them.\n",
    "\n",
    "Recall from week 6 that we can calculate the optimal coefficients for linear regression explicitly using linear algebra. For feature matrix X:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}1&x_{11}&x_{12}&...&x_{1p}\\\\1&x_{21}&x_{22}&...&x_{2p}\\\\ & &...\\\\1&x_{n1}&x_{n2}&...&x_{np}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And label vector y:\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}y_1\\\\y_2\\\\...\\\\y_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can use the formula\n",
    "\n",
    "$$ \\beta = \\left( X^T X\\right)^{-1} X^T \\mathbf y$$\n",
    "\n",
    "The trick to using MapReduce for this problem is noticing that $X^TX$ and $X^Ty$ for the whole dataset can be written as the sum of $X_i^TX_i$ and $X_i^Ty_i$ respectively, where the subscript i refers to individual observations, or subsets of observations.\n",
    "\n",
    "We can decompose Linear Regression by dividing the data into subsets and computing the major matrix multiplication on each of the set. These are later recombined and inverted. See the slides for more details.\n",
    "\n",
    "To pass data in mapreduce it must be in plaintext. So we have to convert our usual DataFrame to a text format such as JSON between each step.\n",
    "\n",
    "The following code outputs the estimated parameters of a Linear Regression on the BatonRouge dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def map_linear_fn(chunk):\n",
    "    # Get the dependant variable y\n",
    "    y = chunk['Price'].values\n",
    "\n",
    "    # Get the independant/feature variables\n",
    "    # which is everything except the price column\n",
    "    X_vals = chunk[chunk.columns.difference(['Price'])].values\n",
    "\n",
    "    # Get the number of data points\n",
    "    m = chunk.shape[0]\n",
    "\n",
    "    # Insert a column of \"1\"s for the intercept term \n",
    "    X = np.column_stack((np.ones(m), X_vals))\n",
    "\n",
    "    # Convert to matrix to make multiplication easier\n",
    "    X = np.asmatrix(X)\n",
    "\n",
    "    # Calculate required multiplications\n",
    "    XtX = X.T*X\n",
    "    Xty = X.T * y.reshape(m,1)\n",
    "\n",
    "    # Yield the result\n",
    "    yield(\"result\", [XtX, Xty])\n",
    "\n",
    "def reduce_linear_fn(key, values):\n",
    "\n",
    "    # Create lists to accumulate the matrices/vectors in\n",
    "    XtX_list = []\n",
    "    Xty_list = []\n",
    "\n",
    "    for result_list in values:\n",
    "        XtX_list.append(result_list[0])\n",
    "        Xty_list.append(result_list[1])\n",
    "\n",
    "    # Sum up all the XtX matrices\n",
    "    XtX = np.asmatrix(sum(XtX_list))\n",
    "\n",
    "    # Sum up all the Xty vectors\n",
    "    Xty = sum(Xty_list)\n",
    "\n",
    "    # Solve the linear regression objective\n",
    "    betas = np.linalg.inv(XtX) * Xty\n",
    "\n",
    "    yield (key, betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('result', matrix([[-4.91975489e+04],\n",
      "        [-4.40598830e+02],\n",
      "        [ 3.98305235e+04],\n",
      "        [-2.56557205e+04],\n",
      "        [-2.13140882e+01],\n",
      "        [-2.94581106e+03],\n",
      "        [ 7.81085610e+03],\n",
      "        [-1.63093297e+03],\n",
      "        [ 8.54451540e+01],\n",
      "        [ 1.06961744e+03],\n",
      "        [ 5.62042730e+04]]))]\n"
     ]
    }
   ],
   "source": [
    "from mockr import run_pandas_job\n",
    "\n",
    "df = pd.read_excel(\"BatonRouge.xls\")\n",
    "\n",
    "results = run_pandas_job(df, map_linear_fn, reduce_linear_fn, n_chunks = 4)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.91975489e+04 -4.40598830e+02  3.98305235e+04 -2.56557205e+04\n",
      " -2.13140882e+01 -2.94581106e+03  7.81085610e+03 -1.63093297e+03\n",
      "  8.54451540e+01  1.06961744e+03  5.62042730e+04]\n"
     ]
    }
   ],
   "source": [
    "# Simplify the result to make it more readable\n",
    "params_mr = np.array(results[0][1]).ravel()\n",
    "\n",
    "print(params_mr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with sklearn parameters\n",
    "\n",
    "Note that even though we seperated the data into multiple chunks the final result is exactly the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.91975489e+04 -4.40598830e+02  3.98305235e+04 -2.56557205e+04\n",
      " -2.13140882e+01 -2.94581106e+03  7.81085610e+03 -1.63093297e+03\n",
      "  8.54451540e+01  1.06961744e+03  5.62042730e+04]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_obj = LinearRegression()\n",
    "\n",
    "features = df[df.columns.difference(['Price'])]\n",
    "target = df['Price']\n",
    "\n",
    "lr_obj.fit(features, target)\n",
    "\n",
    "params_sk = np.append(np.array(lr_obj.intercept_), lr_obj.coef_)\n",
    "\n",
    "print(params_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-bottom: 30px;\"><img width=48 style=\"display: block; float: left;  margin-right: 20px;\" src=\"img/question-mark-button.png\"> <h3 style=\"padding-top: 15px;\">Exercise 2 - Linear Regression and MapReduce</h3></div>\n",
    "\n",
    "In the equation $ \\beta = \\left( X^T X\\right)^{-1} X^T \\mathbf y$:\n",
    "\n",
    "- What are the dimensions of $X^TX$?\n",
    "- What about $X^Ty$?\n",
    "\n",
    "2. Check using Numpy that the product $$X^TX = \\begin{bmatrix}1&2&3\\\\ 4&5&6\\end{bmatrix}\\begin{bmatrix}1&4\\\\ 2&5\\\\ 3&6\\end{bmatrix}$$ can be written as the sum \n",
    "\n",
    "$$\\begin{bmatrix}3\\\\ 6\\end{bmatrix} \\begin{bmatrix}3& 6\\end{bmatrix} + \\begin{bmatrix}2\\\\ 5\\end{bmatrix} \\begin{bmatrix}2& 5\\end{bmatrix} + \\begin{bmatrix}1\\\\ 4\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
